# ğŸ“ å­¦æœ¯è®ºæ–‡
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='{{ "/images/hi-neus.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[High-Fidelity Mask-free Neural Surface Reconstruction for Virtual Reality](https://arxiv.org/abs/2409.13158)

**Haotian Bai**, Yize Chen, Lin Wang

[**é¡¹ç›®ä¸»é¡µ**](https://vlislab22.github.io/Hi-NeuS/) <strong><span class='show_paper_citations' data='DIy4cA0AAAAJ:_FxGoFyzp5QC'></span></strong>\| [**è§†é¢‘**](https://youtu.be/hrkM5N7AltY) \| [![](https://img.shields.io/github/stars/hbai98/Hi_NeuS?style=social)](https://github.com/hbai98/Hi_NeuS)

- ä¸€ç§æ–°é¢–çš„åŸºäºæ¸²æŸ“çš„ç¥ç»éšå¼è¡¨é¢é‡å»ºæ¡†æ¶ï¼Œæ—¨åœ¨**æ— éœ€å¤šè§†è§’ç‰©ä½“æ©ç **çš„æƒ…å†µä¸‹æ¢å¤ç´§å‡‘ä¸”ç²¾ç¡®çš„è¡¨é¢ã€‚
- ç”±äºå›¾åƒä¸­çš„é‡å åŒºåŸŸéšå¼åœ°æ ‡è¯†äº†ç”¨æˆ·æƒ³è¦æ•è·çš„è¡¨é¢ï¼ŒHi-NeuSåˆ©ç”¨å¤šè§†è§’æ¸²æŸ“æƒé‡ä»¥**è‡ªç›‘ç£**æ–¹å¼æŒ‡å¯¼ç¥ç»è¡¨é¢çš„æœ‰ç¬¦å·è·ç¦»å‡½æ•°ã€‚
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='{{ "/images/componerf.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout](https://arxiv.org/abs/2303.13843)

**Haotian Bai**, Yuanhuiyi Lyu, Lutao Jiang,
Sijia Li, Haonan Lu, Xiaodong Lin, Lin Wang

[**é¡¹ç›®ä¸»é¡µ**](https://vlislab22.github.io/componerf/) <strong><span class='show_paper_citations' data='DIy4cA0AAAAJ:YsMSGLbcyi4C'></span></strong>\| [**è§†é¢‘**](https://www.youtube.com/watch?v=eufdSsa-P9U) \| [![](https://img.shields.io/github/stars/hbai98/Componerf?style=social)](https://github.com/hbai98/Componerf)

- ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆæ–‡æœ¬æè¿°ä¸åŸºäºæ¡†çš„ç©ºé—´å¸ƒå±€æ¥åˆæˆè¿è´¯çš„**å¤šç‰©ä½“**åœºæ™¯ã€‚
- CompoNeRFä¸“ä¸ºç²¾åº¦å’Œé€‚åº”æ€§è€Œè®¾è®¡ï¼Œå…è®¸ç”¨å”¯ä¸€æç¤ºé¢œè‰²è¡¨ç¤ºçš„å•ç‹¬NeRFè¿›è¡Œ**ç»„åˆ**ã€**åˆ†è§£**å’Œ**é‡ç»„**ï¼Œä»è€Œåœ¨åˆ†è§£åä»ç¼“å­˜æ¨¡å‹ç®€åŒ–å¤æ‚åœºæ™¯çš„æ„å»ºã€‚
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='{{ "/images/DOT.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF](https://arxiv.org/abs/2307.15333)

**Haotian Bai**, Yiqi Lin, Yize Chen, Lin Wang

[**é¡¹ç›®ä¸»é¡µ**](https://vlislab22.github.io/DOT/) <strong><span class='show_paper_citations' data='DIy4cA0AAAAJ:Y0pCki6q_DkC'></span></strong>\| [**è§†é¢‘**](https://www.youtube.com/watch?v=i9MnoFhH8Ec) \| [![](https://img.shields.io/github/stars/hbai98/DOT?style=social)](https://github.com/hbai98/DOT)

- ä¸€ç§æ›´ç´§å‡‘ä¸”æ›´ä¸°å¯Œçš„PlenOctree (POT) NeRFè¡¨ç¤ºã€‚
- **åˆ›æ–°ç‚¹**ï¼šPOTç”¨äºç›´æ¥ä¼˜åŒ–çš„å›ºå®šç»“æ„æ˜¯æ¬¡ä¼˜çš„ï¼Œå› ä¸ºåœºæ™¯å¤æ‚åº¦éšç€ç¼“å­˜é¢œè‰²å’Œå¯†åº¦çš„æ›´æ–°ä¸æ–­æ¼”å˜ï¼Œéœ€è¦ç›¸åº”åœ°ç»†åŒ–é‡‡æ ·åˆ†å¸ƒä»¥æ•è·ä¿¡å·å¤æ‚åº¦ã€‚
- **æ€§èƒ½**ï¼šDOTåœ¨NeRF-syntheticå’ŒTanks and Templesæ•°æ®é›†ä¸Šè¶…è¶ŠPOTï¼Œæå‡äº†è§†è§‰è´¨é‡ï¼Œå‚æ•°é‡å‡å°‘è¶…è¿‡55.15%/68.84%ï¼Œå¸§ç‡æå‡1.7/1.9å€ã€‚
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='{{ "/images/PatchMix.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Patch-Mix Transformer  for  Unsupervised Domain Adaptation:  A Game Perspective](https://arxiv.org/abs/2303.13434)

Jinjing Zhu*, **Haotian Bai<sup>*</sup>**, Lin Wang

[**é¡¹ç›®ä¸»é¡µ**](https://vlis2022.github.io/cvpr23/PMTrans.html) <strong><span class='show_paper_citations' data='DIy4cA0AAAAJ:UeHWp8X0CEIC'></span></strong>\| [**è§†é¢‘**](https://www.youtube.com/watch?v=WNFlX0WFAO8) \| [![](https://img.shields.io/github/stars/jinjingZhu/PMTrans?style=social)](https://github.com/JinjingZhu/PMTrans)

- è¢«é€‰ä¸ºCVPR <span style="color:red">(äº®ç‚¹)</span>è®ºæ–‡ï¼ˆ**å‰2.5%**ï¼‰
- **å¤§åŸŸå·®è·**ï¼šPMTransé€šè¿‡ä¸­é—´åŸŸä»¥ç›¸å¯¹å¹³æ»‘çš„æ–¹å¼è¿æ¥æºåŸŸå’Œç›®æ ‡åŸŸã€‚
- **åšå¼ˆè®º**ï¼šå°†UDAè§£é‡Šä¸ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå‚ä¸è€…ï¼ˆç‰¹å¾æå–å™¨ã€åˆ†ç±»å™¨å’ŒPatchMixï¼‰çš„æœ€å°-æœ€å¤§äº¤å‰ç†µåšå¼ˆï¼Œä»¥å¯»æ‰¾çº³ä»€å‡è¡¡ã€‚
- **æ€§èƒ½**ï¼šPMTransåœ¨Office-Homeä¸Šè¶…è¶ŠåŸºäºViTå’ŒCNNçš„æœ€ä¼˜æ–¹æ³•+3.6%ï¼Œåœ¨Office-31ä¸Š+1.4%ï¼Œåœ¨DomainNetä¸Š+17.7%ã€‚

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='{{ "/images/SCM.png" | relative_url }}' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration](https://arxiv.org/pdf/2207.10447)

**Haotian Bai**, Ruimao Zhang, Jiong Wang, Xiang Wan

[**é¡¹ç›®ä¸»é¡µ**](https://github.com/hbai98/SCM) <strong><span class='show_paper_citations' data='DIy4cA0AAAAJ:W7OEmFMy1HYC'></span></strong>\| [**è§†é¢‘**](https://www.youtube.com/watch?v=zQdUudmTPOQ) \| [![](https://img.shields.io/github/stars/hbai98/SCM?style=social)](https://github.com/hbai98/SCM)

- SCMæ˜¯å¼±ç›‘ç£ç›®æ ‡å®šä½çš„å¤–éƒ¨Transformerè§£å†³æ–¹æ¡ˆã€‚
- **è½»é‡çº§**ï¼šSCMæ˜¯ä¸€ä¸ªå¤–éƒ¨Transformeræ¨¡å‹ï¼Œä¸äº§ç”Ÿé¢å¤–å‚æ•°ã€‚
- **æ€§èƒ½**ï¼šSCMä»…ä½¿ç”¨çº¦**20%~30%**çš„å‚æ•°å°±è¶…è¶Šäº†å¤§å¤šæ•°ç«äº‰æ€§æ¡†æ¶ï¼ˆCNNå’ŒTransformerï¼‰ã€‚

</div>
</div>

- `NeurIPS 2022`<span style="color:red">(å£å¤´æŠ¥å‘Š)</span> [AMOS: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation.](http://www.amos.sribd.cn/)ï¼ŒYuanfeng Ji, **Haotian Bai**, Jie Yang, Chongjian Ge, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luoã€‚<strong><span class='show_paper_citations' data='DIy4cA0AAAAJ:u-x6o8ySG0sC'></span></strong>
